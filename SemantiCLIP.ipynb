{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8d8d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "# !pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bde62e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certificats SSL contourn√©s pour le t√©l√©chargement.\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "import os\n",
    "\n",
    "# D√©sactive la v√©rification SSL pour le t√©l√©chargement\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "print(\"Certificats SSL contourn√©s pour le t√©l√©chargement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e15c0a",
   "metadata": {},
   "source": [
    "### Initialisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e4be90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device utilis√© : mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 338M/338M [00:11<00:00, 30.6MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from torchvision.datasets import Food101\n",
    "\n",
    "# Configuration du device pour MacBook (M1, M2, M3)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Device utilis√© : {device}\")\n",
    "\n",
    "# C'est ici qu'on d√©finit 'preprocess'\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd0493c",
   "metadata": {},
   "source": [
    "### T√©l√©chargement et Filtrage de Food-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adc9c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Food101\n",
    "import torch\n",
    "\n",
    "# # 1. T√©l√©chargement (seulement si n√©cessaire)\n",
    "# full_train_ds = Food101(root=\"./data\", split=\"train\", download=True, transform=preprocess)\n",
    "# full_test_ds = Food101(root=\"./data\", split=\"test\", download=True, transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c814982a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Cr√©ation du dataset filtr√© (train)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 72199/75750 [01:29<00:04, 810.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Cr√©ation du dataset filtr√© (test)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 24049/25250 [00:31<00:01, 766.64it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from torchvision.datasets import Food101, ImageFolder\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration des chemins\n",
    "source_path = \"./data\"\n",
    "filtered_path = \"./data_filtered\"\n",
    "target_classes = ['pizza', 'hamburger', 'sushi', 'tacos', 'apple_pie', \n",
    "                  'ice_cream', 'omelette', 'french_fries', 'guacamole', 'paella']\n",
    "\n",
    "def create_filtered_dataset(split, max_samples=200):\n",
    "    split_path = os.path.join(filtered_path, split)\n",
    "    \n",
    "    if os.path.exists(split_path):\n",
    "        print(f\"‚úÖ Dossier {split} d√©j√† existant dans {filtered_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"üìÇ Cr√©ation du dataset filtr√© ({split})...\")\n",
    "    # On charge le dataset original sans transformation pour copier les fichiers bruts\n",
    "    full_ds = Food101(root=source_path, split=split, download=True)\n",
    "    \n",
    "    class_counts = {c: 0 for c in target_classes}\n",
    "    target_indices = {full_ds.class_to_idx[c]: c for c in target_classes}\n",
    "\n",
    "    for i in tqdm(range(len(full_ds))):\n",
    "        _, label = full_ds[i]\n",
    "        \n",
    "        if label in target_indices:\n",
    "            class_name = target_indices[label]\n",
    "            if class_counts[class_name] < max_samples:\n",
    "                # Cr√©er le dossier de la classe\n",
    "                os.makedirs(os.path.join(split_path, class_name), exist_ok=True)\n",
    "                \n",
    "                # R√©cup√©rer le chemin de l'image originale\n",
    "                img_path = full_ds._image_files[i]\n",
    "                dest_path = os.path.join(split_path, class_name, os.path.basename(img_path))\n",
    "                \n",
    "                # Copier le fichier\n",
    "                shutil.copy(img_path, dest_path)\n",
    "                class_counts[class_name] += 1\n",
    "        \n",
    "        if all(count >= max_samples for count in class_counts.values()):\n",
    "            break\n",
    "\n",
    "# Ex√©cution pour Train et Test\n",
    "create_filtered_dataset(\"train\", max_samples=200)\n",
    "create_filtered_dataset(\"test\", max_samples=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d44431",
   "metadata": {},
   "source": [
    "### Chargement du Dataset filtr√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c638f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes charg√©es : ['apple_pie', 'french_fries', 'guacamole', 'hamburger', 'ice_cream', 'omelette', 'paella', 'pizza', 'sushi', 'tacos']\n",
      "Total images : 2000 (Train) / 500 (Test)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# On utilise le 'preprocess' de CLIP comme transformation\n",
    "train_ds = ImageFolder(root=os.path.join(filtered_path, \"train\"), transform=preprocess)\n",
    "test_ds = ImageFolder(root=os.path.join(filtered_path, \"test\"), transform=preprocess)\n",
    "\n",
    "print(f\"Classes charg√©es : {train_ds.classes}\")\n",
    "print(f\"Total images : {len(train_ds)} (Train) / {len(test_ds)} (Test)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed326542",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Etape 1: Zero-Shot** \n",
    "\n",
    "1. D√©finir les prompts : Cr√©er une liste de phrases comme \"une photo de pizza\".\n",
    "2. Encoder le texte : Transformer ces phrases en vecteurs (embeddings) avec model.encode_text.\n",
    "3. Comparer : Pour chaque image du set de test, calculer sa similarit√© avec tes 10 phrases. La phrase la plus proche donne la pr√©diction.\n",
    "4. Calculer l'Accuracy : Comparer ces pr√©dictions aux vrais labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b36b2e9",
   "metadata": {},
   "source": [
    "#### 1. Pr√©paration des prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8af29345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordre des classes : ['apple_pie', 'french_fries', 'guacamole', 'hamburger', 'ice_cream', 'omelette', 'paella', 'pizza', 'sushi', 'tacos']\n",
      "Exemple de prompt : a photo of apple_pie, a type of food\n"
     ]
    }
   ],
   "source": [
    "# On r√©cup√®re l'ordre exact des classes charg√©es par ImageFolder\n",
    "class_names = test_ds.classes \n",
    "print(f\"Ordre des classes : {class_names}\")\n",
    "\n",
    "# Cr√©ation des prompts avec un template\n",
    "templates = [f\"a photo of {c}, a type of food\" for c in class_names]\n",
    "print(f\"Exemple de prompt : {templates[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df72c010",
   "metadata": {},
   "source": [
    "#### 2. Encodage du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bf0bf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions des features texte : torch.Size([10, 512])\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation et Encodage\n",
    "text_tokens = clip.tokenize(templates).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens)\n",
    "    # Normalisation pour le calcul de la similarit√© cosinus\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "print(f\"Dimensions des features texte : {text_features.shape}\") # [10, 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeb7cf4",
   "metadata": {},
   "source": [
    "#### 3. Comparaison & Calcul de l'Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1cd367a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "√âvaluation Zero-shot: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:05<00:00,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- R√âSULTAT ---\n",
      "Accuracy Zero-shot sur Food-10 : 98.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"√âvaluation Zero-shot\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 1. Encoder l'image\n",
    "        image_features = model.encode_image(images)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # 2. Calculer la similarit√© (Produit scalaire car les vecteurs sont normalis√©s)\n",
    "        # On multiplie par 100 (le logit_scale de CLIP) avant le softmax\n",
    "        logits = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        \n",
    "        # 3. Prendre la classe avec le score le plus haut\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        \n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "zero_shot_accuracy = (correct / total) * 100\n",
    "print(f\"\\n--- R√âSULTAT ---\")\n",
    "print(f\"Accuracy Zero-shot sur Food-10 : {zero_shot_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6e0ec4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Etape 2: Extraction et Stockage des Features**\n",
    "\n",
    "1. Passage unique : Tu passes toutes tes images de train_ds et test_ds dans model.encode_image.\n",
    "2. Conversion NumPy : Tu stockes les r√©sultats dans des tableaux NumPy (train_features et test_features).\n",
    "Pourquoi ? Parce qu'entra√Æner un classifieur sur des vecteurs d√©j√† extraits prend 1 seconde, alors que le faire sur des images prendrait des minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb82758",
   "metadata": {},
   "source": [
    "#### 1. Fonction d'extraction des caract√©ristiques (Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db8a7066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_and_save_features(dataset, filename_prefix):\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    # Utilisation d'un batch_size de 32 pour ne pas saturer la RAM du Mac\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=f\"Extraction {filename_prefix}\"):\n",
    "            # Envoi sur MPS (ou CPU)\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # 1. Passage dans l'encodeur d'images de CLIP\n",
    "            features = model.encode_image(images)\n",
    "            \n",
    "            # 2. Normalisation L2 (essentielle pour la coh√©rence avec CLIP)\n",
    "            features /= features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # 3. Stockage en NumPy (on quitte le GPU/MPS ici)\n",
    "            features_list.append(features.cpu().numpy())\n",
    "            labels_list.append(labels.numpy())\n",
    "            \n",
    "    # On concat√®ne tous les batches\n",
    "    final_features = np.concatenate(features_list, axis=0)\n",
    "    final_labels = np.concatenate(labels_list, axis=0)\n",
    "    \n",
    "    # Sauvegarde sur le disque\n",
    "    np.save(f\"{filename_prefix}_features.npy\", final_features)\n",
    "    np.save(f\"{filename_prefix}_labels.npy\", final_labels)\n",
    "    \n",
    "    return final_features, final_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bf5bdc",
   "metadata": {},
   "source": [
    "#### 2. Ex√©cution et Stockage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d41af13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ D√©but de l'extraction massive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:19<00:00,  3.18it/s]\n",
      "Extraction test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Extraction termin√©e et fichiers sauvegard√©s !\n",
      "Dimensions Train : (2000, 512)\n",
      "Dimensions Test : (500, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ D√©but de l'extraction massive...\")\n",
    "\n",
    "# Extraction pour le Train (200 images par classe)\n",
    "train_features, train_labels = extract_and_save_features(train_ds, \"train\")\n",
    "\n",
    "# Extraction pour le Test (50 images par classe)\n",
    "test_features, test_labels = extract_and_save_features(test_ds, \"test\")\n",
    "\n",
    "print(\"\\n‚úÖ Extraction termin√©e et fichiers sauvegard√©s !\")\n",
    "print(f\"Dimensions Train : {train_features.shape}\") # Devrait √™tre [2000, 512]\n",
    "print(f\"Dimensions Test : {test_features.shape}\")   # Devrait √™tre [500, 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78821202",
   "metadata": {},
   "source": [
    "### Etape 3: Sc√©narios Linear Probing (1-shot et 5-shots)\n",
    "\n",
    "1. S√©lection des donn√©es (le \"N-shot\") :\n",
    "    - Pour le 1-shot : Tu parcours ton set d'entra√Ænement et tu prends exactement 1 image au hasard pour chacune des 10 classes.\n",
    "    - Pour le 5-shots : Tu prends 5 images par classe.\n",
    "\n",
    "2. Entra√Ænement : Tu utilises une LogisticRegression (de Scikit-Learn) ou une couche Linear (PyTorch). Tu l'entra√Ænes √† faire le lien entre les features extraites √† l'√©tape 2 et les labels.\n",
    "\n",
    "3. √âvaluation : Tu testes ce classifieur sur tes test_features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a91fa4",
   "metadata": {},
   "source": [
    "### Etape 4: Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539b7d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
